- proxy requisiti e constraint bug 
- cache come funziona
- gestione SSL 
- processo di onboarding strategie e differenze tra le strategie
- Gestione end point e requisiti hardware server e proxy


## Proxy requisiti e constraint bug 

Il bug è tracciato come SUSE Bugzilla **bsc#1246789** - "Use existing systemid in proxy httpd if present".
Il fix è stato implementato in due componenti:

1. uyuni-tools (mgrpxy) - https://github.com/uyuni-project/uyuni-tools/pull/642 - "Add automatic systemid refresh" -  Merged il 19 agosto 2025, rilasciato in uyuni-tools v5.1.20 e v5.2.1
2. proxy-httpd-image v5.1.9 - Il container gestisce correttamente il systemid esistente invece di crashare

State usando UYUNI 2025.10 con immagini dal registry registry.opensuse.org/uyuni/proxy-httpd:latest. Se le immagini non includono ancora il fix, le opzioni sono:

- Aggiornare mgrpxy e le immagini container a versioni >= 5.1.20 / 5.1.9 (il fix dovrebbe essere incluso)
- Applicare il workaround manuale che abbiamo documentato (mkdir + volume mount + permessi)

Non serve aprire un nuovo bug

Dato che è già fixato, non serve segnalarlo. Se però il problema persiste anche dopo l'aggiornamento, si può aprire una regression issue su https://github.com/uyuni-project/uyuni/issues/new referenziando bsc#1246789.

Quando nella documentazione avevo scritto "bug più o meno noto", mi basavo sul fatto che il comportamento (il container che crasha per **FileNotFoundError** su systemid) è un pattern tipico di volume mount mancante nei deployment containerizzati, e lo script uyuni-configure.py mostrava chiaramente che il systemid doveva essere scritto in una directory non montata. Non avevo una reference specifica in quel momento - la ricerca che ho fatto ora ha confermato che il bug era effettivamente tracciato e fixato.

Consiglio: verificate la versione di mgrpxy installata sul proxy (**mgrpxy --version** o **rpm -q mgrpxy**) per capire se il fix è già incluso nella vostra installazione.

  Sources:
  - https://github.com/uyuni-project/uyuni-tools/pull/642
  - https://www.suse.com/releasenotes/x86_64/multi-linux-manager-proxy/5.1/index.html
  - https://github.com/uyuni-project/uyuni/issues

Quando viene installato un nuovo proxy uyuni, da procedura viene eseguita tramite **mgrpxy** install podman, che è un tool che genera automaticamente i servizi **systemd** per i 5 container. Uno di questi, proxy-httpd, è quello che gestisce tutte le richieste HTTP/HTTPS dei client (download dei pacchetti, metadata dei repository, ecc.), per questo falliva con gli errata. All’avvio, proxy-httpd esegue uno script Python (uyuni-configure.py) che deve scrivere un file chiamato **systemid** nella directory /etc/sysconfig/rhn/. Questo file contiene le credenziali XML con cui il proxy si autentica verso il server UYUNI ogni volta che un client richiede un pacchetto.

Il “bug” è che il servizio systemd generato da mgrpxy non crea il mount per /etc/sysconfig/rhn/. Quindi lo script tenta di scrivere il file ma, non trovando la directory, il container va in crash in un loop di riavvio. Per risolvere, ho aggiunto alla procedura di installazione la creazione manuale della directory /etc/sysconfig/rhn/ sull’host e la riga -v /etc/sysconfig/rhn:/etc/sysconfig/rhn nel file di servizio systemd, in modo che il container veda la directory dell’host. A quel punto lo script riesce a scrivere il systemid e il proxy funziona. Bisogna creare solo la directory, non il file, perché il tool controlla l’eventuale esistenza del file alla prima esecuzione e lo genera se necessario. Ovviamente, creando la directory manualmente, è necessario impostare anche i permessi corretti.

> Il Proxy DEVE risolvere l'FQDN (Fully Qualified Domain Name) del Server UYUNI. Aggiungere anche l'entry del Server nel file hosts se non si usa Azure Private DNS Zone.
## Preparare l'Host Proxy per la Comunicazione Salt

> NON eseguire il bootstrap Salt separatamente prima della FASE 8. Il comando `proxy_container_config_generate_cert` nella FASE 8 crea una registrazione tradizionale (systemid) con un checksum. Se si esegue il bootstrap Salt PRIMA o DOPO la generazione del config, il bootstrap modifica il checksum sul server causando un mismatch: il proxy tenta di autenticarsi con il checksum del config, ma il server si aspetta quello del bootstrap, risultando in errore `Invalid System Credentials` e HTTP 500 su ogni richiesta dei client.
## Cache come funziona

**Squid** è il proxy HTTP/HTTPS di caching usato per fare da intermediario tra i client e i repository remoti. 

> La dimensione del disco cache Squid determina quanti pacchetti vengono serviti localmente senza contattare il Server. Più grande è, meno traffico di rete tra Proxy e Server. Impostare Squid cache al massimo **60% dello spazio disponibile** sul disco cache.
## Architettura: Uyuni Server → Proxy → Client

## Ruolo di Squid dentro Uyuni Proxy

Nel proxy Uyuni, Squid svolge 3 funzioni tecniche principali:

### 1) Caching dei contenuti dei repository
Quando un client richiede un pacchetto (RPM, metadata, patch):

1. Il client chiede al Proxy.
2. Squid verifica se l’oggetto è in cache.
3. Se presente → serve localmente (cache hit).
4. Se assente → lo scarica dall’Uyuni Server (o upstream) e lo memorizza.

**Vantaggi:**

* Riduzione drastica della banda WAN
* Aggiornamenti più rapidi per molti client
* Scalabilità per sedi remote

Tipicamente cache di:

* RPM / DEB
* Metadata repo (repodata)
* File bootstrap
### 2) Reverse proxy verso Uyuni Server
Squid nel proxy agisce anche come **accelerator/reverse proxy** per:

* `/rhn/` (API e contenuti Uyuni)
* `/pub/` (bootstrap e chiavi)
* `/os-images/` (se usi provisioning PXE)

Questo permette ai client di vedere **solo il proxy**, non il server centrale.
### 3) Terminazione TLS e controllo accessi
Configurazione standard Uyuni Proxy include:

* Certificati SSL firmati dall’Uyuni Server
* ACL Squid per limitare chi può usare il proxy
* Porte dedicate (tipicamente 80/443/5222 per vari servizi)

## Gestione SSL

Il problema della **gestione delle chiavi SSL** tra **Server ↔ Proxy ↔ Minion** in **Uyuni** è uno dei punti più critici dell’architettura, perché coinvolge **PKI interna, fiducia TLS e bootstrap dei client**. 

# Architettura delle chiavi SSL in Uyuni

Uyuni usa una **PKI propria** (Certificate Authority interna):

```
Uyuni CA
 ├── Certificato Server Uyuni
 ├── Certificati Proxy Uyuni
 └── Certificati Client (minion)
```

Ogni componente deve fidarsi della CA.
# Flussi di comunicazione coinvolti
### 1) Minion → Proxy
Il minion stabilisce TLS verso il proxy:

* verifica il certificato del proxy
* deve avere installata la CA Uyuni

Se la CA non coincide → errore TLS.
### 2) Proxy → Server Uyuni
Il proxy a sua volta:

* si autentica verso il server
* usa certificati firmati dalla stessa CA

Se il proxy ha certificati vecchi o di un’altra CA → il server rifiuta.
### 3) Bootstrap del Minion
Durante bootstrap:
1. Il minion scarica:
   * certificato CA
   * configurazione repo
2. Registra la chiave Salt
3. Stabilisce comunicazione permanente

Se bootstrap usa il server ma poi il client parla col proxy → mismatch.
# Problemi tipici (i più comuni)

## ❌ 1) CA diversa tra Server e Proxy

Succede quando:

* reinstalli il proxy
* ricrei certificati
* cloni macchine

Sintomi:

* client non si registrano
* download pacchetti fallisce
* errori TLS handshake
## ❌ 2) Certificati scaduti

Uyuni non rinnova automaticamente tutti i certificati.

Sintomi:

* proxy apparentemente attivo ma inutilizzabile
* errori tipo:

  ```
  certificate verify failed
  ```

---
# Best practice operative
## ✔️ 1) Generare certificati dal Server

Sempre dal server Uyuni:

```
mgr-ssl-cert-setup
mgr-proxy
```
## ✔️ 2) Usare FQDN coerenti

I certificati includono hostname.

Se cambi hostname → invalidi i certificati.

---
## ✔️ 3) Rigenerare bootstrap dopo modifiche

Ogni volta che cambi proxy o certificati:

* rigenera bootstrap repo
* usa script aggiornati

---
## ✔️ 4) Verificare la catena di fiducia

Dal minion:

```
openssl s_client -connect proxy:443
```

## Processo di onboarding strategie e differenze tra le strategie

Come preparare l’onboarding di sistemi **RHEL 9 tramite proxy Uyuni**, affrontando problemi reali emersi durante i test:

## Obiettivi principali
1. Creare il **Bootstrap Repository** per RHEL9
2. Risolvere incompatibilità del pacchetto `venv-salt-minion` con OpenSSL
3. Generare uno script bootstrap specifico per proxy
4. Configurare correttamente Activation Key
5. Verificare connettività, entropia e prerequisiti SSH

# Bootstrap Repository per RHEL9

## 1. Verifica repository bootstrap disponibili
```bash
mgrctl exec -- mgr-create-bootstrap-repo --list
```
### Cosa fa
- Esegue nel container server il tool che elenca i bootstrap repo generabili.
- `mgrctl exec --` serve quando Uyuni è installato containerizzato.
### Output atteso
```
1. RHEL9-x86_64-uyuni
2. ubuntu-24.04-amd64-uyuni
```
### Significato
Mostra quali bootstrap repo possono essere creati.
## 2. Creazione bootstrap repo RHEL9

```bash
mgrctl exec -- mgr-create-bootstrap-repo --create=RHEL9-x86_64-uyuni
```
### Cosa fa
Genera:
- repo bootstrap
- pacchetti necessari alla registrazione
- struttura web servita dal proxy/server

Serve perché i client usano questo repo PRIMA di registrarsi.
## 3. Verifica creazione repository

```bash
mgrctl exec -- ls /srv/www/htdocs/pub/repositories/
```
### Cosa fa
Lista le directory dei repo bootstrap pubblicati via HTTP.
### Output atteso

```
res  ubuntu
```

### Significato

- `res` → repository RHEL/SUSE-like
- `ubuntu` → repository Ubuntu

Se `res` esiste, il repo RHEL è pronto.
# 4. Problema venv-salt-minion vs OpenSSL

## Verifica versioni presenti

```bash
mgrctl exec -- ls -la /srv/www/htdocs/pub/repositories/res/9/bootstrap/x86_64/
```
### Cosa fa
Mostra i pacchetti bootstrap disponibili per RHEL9 x86_64.
### Output tipico

```
venv-salt-minion-3006.0-47.36.uyuni.x86_64.rpm
venv-salt-minion-3006.0-58.1.uyuni.x86_64.rpm
```
### Interpretazione
- Versione 47.36 → compatibile OpenSSL 3.0.x
    - Versione 58.1 → richiede OpenSSL ≥ 3.3
    
Se entrambe presenti, il bootstrap potrebbe installare quella sbagliata.
## Rimozione versione incompatibile

```bash
mgrctl exec -- rm /srv/www/htdocs/pub/repositories/res/9/bootstrap/x86_64/venv-salt-minion-3006.0-58.1.uyuni.x86_64.rpm
```

### Cosa fa
Elimina il pacchetto incompatibile.
### Output atteso
Nessun output se successo.

## 5. generazione metadata repo

```bash
mgrctl exec -- createrepo_c /srv/www/htdocs/pub/repositories/res/9/bootstrap/
```
### Cosa fa
Ricrea:
- repodata
- indici RPM    

Senza questo passo il repo sarebbe inconsistente.
### Output tipico

```
Spawning worker 0 with 4 pkgs
...
Saving Primary metadata
```
# Script Bootstrap via Proxy

## 6. Generazione script bootstrap

```bash
mgrctl exec -- mgr-bootstrap \
  --hostname=uyuni-proxy-test.uyuni.internal \
  --activation-keys=1-rhel9 \
  --script=bootstrap-rhel9-proxy.sh
```
### Cosa fa
Crea uno script che:
- installa certificati
- configura repo bootstrap
- registra il minion tramite proxy
### Parametri
- `--hostname` → endpoint che useranno i client (proxy)
- `--activation-keys` → profilo di registrazione 
- `--script` → nome file generato
### Output atteso
Messaggio di creazione script, ad esempio:

```
Bootstrap script written to:
 /srv/www/htdocs/pub/bootstrap/bootstrap-rhel9-proxy.sh
```

```bash
ssh azureuser@10.172.2.21 "curl -Sks https://uyuni-proxy-test.uyuni.internal/pub/bootstrap/bootstrap-rhel9-proxy.sh | sudo bash"
```
# Activation Key — Verifica via CLI

## 7. Controllo configurazione Activation Key

```bash
mgrctl exec -- spacecmd -u admin -p '<ADMIN_PASS>' -- activationkey_details 1-rhel9
```

Usa **spacecmd**
### Cosa fa
Mostra:
- canali assegnati
- permessi
- system types
### Output atteso
Se configurata correttamente vedrai:
- Config actions: enabled
- Remote commands: enabled
- Monitoring: enabled

## Tabella Comparativa Metodi

| #     | Metodo                | Via Proxy         | Richiede SSH diretto | Parallelismo      | Caso d'uso               |
| ----- | --------------------- | ----------------- | -------------------- | ----------------- | ------------------------ |
| **1** | **Bootstrap + SSH**   | Si                | Si (dal workstation) | Sì(xargs/pssh)    | **Scenario principale**  |
| **2** | **spacecmd**          | Si (nativo)       | No (dal server)      | Sequenziale       | No SSH diretto ai target |
| **3** | **API XML-RPC**       | Si (con proxy_id) | No (dal server)      | Sequenziale       | Automazione/integrazione |
| **4** | **Azure Run Command** | Si                | No (usa Guest Agent) | Buono (--no-wait) | Ambiente Azure nativo    |


**spacecmd** è un **client a riga di comando** per amministrare **Uyuni** tramite API XML-RPC. In pratica è una **CLI amministrativa** che permette di gestire server, canali software, patch, gruppi e configurazioni senza usare l’interfaccia web.
# A cosa serve (operativamente)

Con spacecmd puoi:
## Gestione sistemi

* elencare sistemi registrati
* assegnare canali software
* applicare patch
* aggiungere a system groups

Esempi tipici:

```
spacecmd system_list
spacecmd system_details nomehost
spacecmd system_scheduleapplyerrata nomehost
```

