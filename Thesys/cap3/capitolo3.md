# Capitolo 3 — Metriche per i Trade-off di Sicurezza nel Patch Management

---

## 3.1 Quality Assurance e Misurazione delle Performance nel Patch Management

La gestione delle patch è storicamente concepita come un'attività operativa di natura binaria: una patch viene applicata oppure non lo è. Questa visione, per quanto intuitivamente comprensibile, nasconde la complessità reale del processo e, soprattutto, priva l'organizzazione di qualsiasi strumento per valutare l'efficacia complessiva del programma nel tempo. Il passaggio concettuale che la letteratura scientifica più recente ha compiuto consiste nell'applicare i principi della **Quality Assurance (QA)** al dominio della cybersecurity: non si tratta di verificare se una singola patch è stata installata, ma di misurare con continuità quanto bene l'intera catena di processo gestisca il rischio derivante dalle vulnerabilità note. Ojuri (2021) dimostra empiricamente come questo cambio di paradigma produca miglioramenti significativi e riproducibili, con il tasso di successo del deployment che cresce dal 72% al 90%, il tempo medio di applicazione che si riduce di un terzo e la frequenza di rollback che cala di oltre il 60% nei programmi che adottano indicatori QA strutturati rispetto a quelli che operano senza misurazione sistematica [^1].

Il principio cardine della QA applicata alla sicurezza è che ciò che non si misura non si può migliorare, e che le metriche devono essere selezionate non per la facilità di raccolta, ma per la loro capacità di segnalare deviazioni rispetto a un obiettivo di rischio dichiarato. In questo senso, la letteratura di settore e i framework di riferimento convergono su cinque indicatori chiave che, considerati nel loro insieme, forniscono una visione multidimensionale della qualità del processo di patch management [^1][^2][^3].

Il primo e più immediato di questi indicatori è il **Patch Success Rate (PSR)**, definito come la percentuale di patch deployate con successo sul totale dei tentativi effettuati. Il PSR misura la reliability del processo di deployment ed è sensibile alla qualità dei test condotti in ambienti di staging prima della produzione, alla gestione delle dipendenze applicative e alla maturità delle procedure operative. Organizzazioni con elevata maturità QA raggiungono stabilmente valori superiori al 95%, mentre quelle prive di framework strutturati si attestano intorno all'82% [^1]. Un PSR sistematicamente basso non è un problema tecnico ma un segnale di debolezza strutturale del processo: testing insufficiente, scarsa governance delle dipendenze, assenza di ambienti di validazione pre-produzione.

Strettamente connesso al PSR, ma orientato alla dimensione temporale, è il **Mean Time to Deploy (MTTD)**, che misura l'intervallo medio tra la disponibilità pubblica di una patch e la sua effettiva applicazione sui sistemi in produzione. Questo intervallo rappresenta la **finestra di esposizione**: il periodo durante il quale una vulnerabilità nota può essere sfruttata da un attaccante che abbia accesso alle stesse informazioni pubbliche del defender. Il NIST SP 800-40r4 raccomanda finestre differenziate in base alla criticità della vulnerabilità, con patch critiche da applicare entro 72 ore, importanti entro 30 giorni e moderate entro 90 giorni [^2]. I dati empirici mostrano che l'adozione di strumenti di validazione automatizzata riduce il MTTD del 64%, portando organizzazioni ad alta maturità a valori medi di 12 ore contro le 72 ore di quelle a bassa maturità [^1]. Questa differenza, apparentemente tecnica, si traduce in una differenza sostanziale di esposizione al rischio: ogni ora di ritardo nella chiusura di una vulnerabilità critica è un'ora in cui un attaccante può agire indisturbato.

La **Rollback Frequency (RF)** misura la percentuale di patch che vengono ritirate dopo il deployment per errori, incompatibilità o effetti indesiderati sui sistemi. Non si tratta di un semplice indicatore di fallimento, ma di un segnale diagnostico prezioso: le rollback, opportunamente categorizzate nelle loro cause radice — errori di configurazione, incompatibilità applicative, errori procedurali — consentono di identificare e correggere sistematicamente le debolezze del processo di QA pre-deployment. Un tasso di rollback dell'1,8% caratterizza i programmi ad alta maturità, mentre valori intorno al 7,5% segnalano processi di testing inadeguati [^1]. La **Vulnerability Remediation Coverage (VRC)** completa il quadro misurando la percentuale di vulnerabilità critiche risolte entro i termini definiti dallo SLA interno: a differenza del PSR, che valuta la qualità del singolo deployment, il VRC misura l'efficacia complessiva del programma rispetto al rischio prioritario, rivelando se la selezione e la prioritizzazione delle patch sono allineate con il threat landscape reale. Il quinto indicatore, il **System Downtime (SD)**, cattura invece il trade-off tra sicurezza e continuità operativa, misurando il tempo di interruzione del servizio causato dalle attività di patching: è l'unica metrica che cresce al crescere della qualità del processo di sicurezza, creando una tensione strutturale che spinge le organizzazioni a ritardare le patch — paradossalmente aumentando il rischio [^1][^3].

Ojuri (2021) formalizza questi cinque indicatori nel **Patch Management QA Score (PMQAS)**, un modello ponderato che li aggrega in un indice sintetico assegnando al PSR un peso del 30%, al MTTD del 25%, e agli ultimi tre indicatori il 15% ciascuno [^1]. Il dato forse più significativo che emerge dalla ricerca riguarda il ruolo della formazione del personale: dopo programmi strutturati di training, il tasso di fallimento delle patch cala dall'8,2% al 3,1%, il tempo medio di recovery dagli incidenti si riduce da 14,6 a 6,3 ore, e la compliance alle policy interne passa dal 72,5% al 94,4% [^1]. Questo risultato conferma che la qualità del patch management non è esclusivamente una questione tecnica, ma dipende in misura determinante dalla competenza e dalla cultura di sicurezza delle persone che operano il processo.

> **Figura consigliata — Fig. 3.1**: Grafico a barre raggruppate che confronta i valori di PSR, MTTD, RF e Vulnerability Exposure tra organizzazioni ad alta e bassa maturità QA. I dati sono estratti dalla Table 3 e Table 5 di Ojuri (2021) [^1] e la figura può essere ridisegnata come grafico originale con strumenti come matplotlib (Python) o ggplot2 (R). Non esistendo una versione open-access del paper, il grafico va prodotto autonomamente dai dati riportati nel testo.

---

## 3.2 La Valutazione Tecnica delle Vulnerabilità: CVSS ed EPSS

La misurazione della qualità del processo di patching definita nella sezione precedente presuppone che esista un criterio affidabile per determinare la pericolosità delle singole vulnerabilità e, di conseguenza, l'ordine di priorità con cui esse devono essere affrontate. In assenza di tale criterio, qualsiasi metrica di processo — per quanto sofisticata — misura la velocità con cui si patchano vulnerabilità selezionate arbitrariamente, senza garanzia alcuna che le risorse siano allocate verso le minacce più urgenti. Due standard si sono imposti come riferimento internazionale per rispondere a questo problema: il CVSS per la severità tecnica intrinseca e l'EPSS per la probabilità di sfruttamento nel mondo reale.

Il **Common Vulnerability Scoring System (CVSS)**, sviluppato e mantenuto dal FIRST (Forum of Incident Response and Security Teams) e adottato universalmente come standard de facto per la classificazione delle vulnerabilità, fornisce un punteggio da 0.0 a 10.0 che esprime la severità tecnica intrinseca di una vulnerabilità attraverso un insieme strutturato di metriche [^5]. La versione 4.0, finalizzata nel novembre 2023, introduce una separazione più granulare tra le metriche base — che catturano caratteristiche immutabili della vulnerabilità come il vettore di attacco, la complessità, i privilegi richiesti e l'impatto sulla triade CIA (Confidenzialità, Integrità, Disponibilità) — e un gruppo di Threat Metrics che incorporano informazioni sulla maturità degli exploit disponibili, avvicinando lo standard alla realtà del threat landscape [^5][^6]. Il sistema prevede inoltre Environmental Metrics che consentono all'organizzazione di calibrare il punteggio in base al proprio contesto specifico, e Supplemental Metrics che includono fattori come l'automabilità dell'attacco e l'impatto potenziale sulla sicurezza fisica.

Nonostante la sua diffusione universale e l'indubbio valore come lingua comune per comunicare la gravità delle vulnerabilità, il CVSS presenta un limite strutturale ben documentato e riconosciuto dagli stessi autori dello standard: essendo una misura del *potenziale* danno, rimane statico nel tempo e non riflette l'effettiva probabilità che una vulnerabilità venga sfruttata da attori malevoli nel contesto operativo reale [^6][^7]. Le conseguenze pratiche di questo limite sono significative: ricerche empiriche condotte su dataset storici dimostrano che soltanto il 2,3% delle vulnerabilità con punteggio CVSS uguale o superiore a 7 viene effettivamente sfruttato nell'arco dei 30 giorni successivi alla pubblicazione [^6]. Affidarsi esclusivamente al CVSS per la prioritizzazione delle patch porta quindi a una sistematica *false prioritization*: i team di sicurezza concentrano risorse limitate su vulnerabilità che nella grande maggioranza dei casi non verranno mai sfruttate, mentre potrebbero trascurare vulnerabilità con punteggi moderati ma oggetto di campagne di attacco attive.

L'**Exploit Prediction Scoring System (EPSS)**, sviluppato dal FIRST con il contributo di un consorzio internazionale di ricercatori, nasce esplicitamente per colmare questo gap [^8]. Il sistema fornisce una stima giornaliera della probabilità che una vulnerabilità venga sfruttata *in the wild* entro i successivi 30 giorni, espressa come valore tra 0.0 e 1.0. La versione 4, rilasciata nel marzo 2025, rappresenta un salto qualitativo rilevante rispetto alle versioni precedenti: il modello di machine learning sottostante viene aggiornato quotidianamente consumando oltre 250.000 data point di threat intelligence, che comprendono feed di exploit pubblici, telemetria da vendor di sicurezza, dati di honeypot e report di incident response [^8][^9]. L'EPSS v4 è inoltre in grado di assegnare punteggi alle CVE ancora in backlog presso la National Vulnerability Database (NVD), fornendo ai defender visibilità su vulnerabilità per le quali l'analisi ufficiale non è ancora disponibile — un vantaggio rilevante dato il noto ritardo accumulato nel backlog NVD negli ultimi anni.

La differenza operativa tra i due sistemi è immediatamente comprensibile attraverso un esempio concreto: una vulnerabilità con CVSS 9.8 ma EPSS 0.01 è significativamente meno urgente di una vulnerabilità con CVSS 7.0 ed EPSS 0.85. Nel primo caso, il sistema è tecnicamente devastante ma raramente oggetto di sfruttamento nella pratica — magari perché richiede accesso fisico al sistema, perché non esistono exploit pubblici affidabili, o perché il vettore di attacco è estremamente complesso da replicare. Nel secondo caso, la vulnerabilità è di gravità moderata ma viene attivamente sfruttata da attori malevoli con alta probabilità nel breve termine. Utilizzare il CVSS come unico criterio di prioritizzazione porterebbe a classificare il primo caso come urgente e il secondo come secondario, invertendo completamente la corretta scala di priorità [^6][^7].

> **Figura consigliata — Fig. 3.2**: Scatter plot con CVSS sull'asse X ed EPSS sull'asse Y per un campione rappresentativo di CVE, che illustra visivamente la distribuzione non correlata dei due punteggi e la presenza di quattro quadranti distinti (alto CVSS/basso EPSS, basso CVSS/alto EPSS, ecc.). Il dataset EPSS è disponibile pubblicamente e liberamente scaricabile all'indirizzo **https://www.first.org/epss/data** in formato CSV. Il grafico può essere generato con Python/matplotlib o R/ggplot2. Una versione di riferimento di questa visualizzazione è presente nel blog di Attaxion [^7] e nel report annuale FIRST EPSS disponibile su **https://www.first.org/epss/articles**.

È opportuno infine richiamare, per completezza del quadro teorico, altri due strumenti complementari che la comunità di sicurezza ha sviluppato per raffinare ulteriormente la prioritizzazione. Il **Known Exploited Vulnerabilities (KEV) Catalog**, mantenuto dalla CISA (Cybersecurity and Infrastructure Security Agency), elenca le vulnerabilità per le quali è documentato uno sfruttamento attivo e confermato nel mondo reale [^10]: una CVE presente nel catalogo KEV costituisce per definizione una minaccia immediata e concreta, indipendentemente dal punteggio CVSS. Lo **Stakeholder-Specific Vulnerability Categorization (SSVC)**, sviluppato congiuntamente da CISA e dal CERT/CC della Carnegie Mellon University, offre invece un framework di prioritizzazione strutturato su alberi decisionali che considera la disponibilità dell'exploit, l'automabilità dell'attacco e l'impatto tecnico, producendo raccomandazioni di azione categoriali anziché punteggi numerici [^10]. L'approccio metodologicamente più robusto per la prioritizzazione delle patch integra questi strumenti in modo complementare: CVSS per l'impatto potenziale, EPSS per la probabilità di sfruttamento nel breve termine, KEV come segnale binario di urgenza immediata, e il contesto aziendale dell'asset come fattore di amplificazione o attenuazione [^9].

---

## 3.3 Il Framework NIST per la Gestione Strutturata del Rischio

La valutazione tecnica delle vulnerabilità, per quanto sofisticata nei suoi strumenti e nei suoi modelli, non esaurisce il problema della gestione della sicurezza. Le decisioni su come rispondere a una vulnerabilità identificata — con quale urgenza, attraverso quali misure, con quale documentazione formale — richiedono un framework organizzativo strutturato che governi il processo decisionale in modo coerente, ripetibile e auditabile. Il National Institute of Standards and Technology (NIST) ha sviluppato nel corso degli anni un ecosistema coerente di standard, linee guida e framework che rappresenta il riferimento internazionale più autorevole per la gestione del rischio in ambito cybersecurity, adottato da organizzazioni governative e private in tutto il mondo.

Il **NIST Risk Management Framework (RMF)**, descritto nelle pubblicazioni NIST SP 800-37r2 e SP 800-39, definisce un processo strutturato in sette fasi sequenziali e iterative che guida un'organizzazione dalla comprensione del proprio contesto fino al monitoraggio continuo dell'efficacia delle misure adottate [^11][^12]. Il framework non è un elenco di controlli tecnici, ma un processo decisionale: stabilisce *come* le organizzazioni devono affrontare il rischio, non *cosa* devono fare tecnicamente. La prima fase, **Prepare**, è fondativa e spesso sottovalutata: consiste nell'istituire il contesto organizzativo necessario per eseguire le fasi successive, definendo ruoli e responsabilità, identificando la *risk appetite* dell'organizzazione — ovvero il livello di rischio che essa è disposta ad accettare nell'esercizio della propria missione — e stabilendo i criteri con cui le decisioni di rischio verranno prese e documentate. La fase di **Categorize** richiede la classificazione dei sistemi informativi e dei dati che essi trattano in base al loro impatto potenziale sulla missione organizzativa, seguendo la metodologia FIPS 199 che distingue tre livelli — Low, Moderate, High — in funzione delle conseguenze di una violazione sulla confidenzialità, integrità e disponibilità delle informazioni. Questa categorizzazione determina la severità delle misure di protezione richieste nelle fasi successive.

Le fasi di **Select**, **Implement** e **Assess** costituiscono il nucleo operativo del framework: i controlli di sicurezza vengono selezionati dal catalogo NIST SP 800-53 in base alla categoria di impatto del sistema, implementati nell'architettura operativa e infine valutati nella loro efficacia reale, determinando se producono i risultati attesi in termini di riduzione del rischio. La fase di **Authorize** rappresenta il momento di sintesi decisionale più critico dell'intero processo: un funzionario senior dell'organizzazione, investito della responsabilità formale, valuta il rischio residuo — quello che rimane dopo l'implementazione di tutti i controlli — e decide esplicitamente se esso è accettabile alla luce della *risk appetite* definita nella fase preparatoria. Questa autorizzazione formalizza un principio fondamentale della gestione del rischio: il rischio accettato non è un'omissione o una negligenza, ma una decisione consapevole, documentata e firmata da chi ha l'autorità e la responsabilità per assumerla. L'ultima fase, **Monitor**, completa il ciclo trasformando il framework in un processo continuo: i controlli implementati, il sistema e il threat landscape vengono monitorati costantemente per rilevare cambiamenti — nuove vulnerabilità, variazioni nella configurazione, evoluzione degli attori minaccia — che richiedano una rivalutazione del rischio e il riavvio del ciclo [^11].

Complementare al RMF nella sua specificità operativa, il **NIST Cybersecurity Framework (CSF) 2.0**, pubblicato nel febbraio 2024, offre un linguaggio comune e accessibile per comunicare la postura di sicurezza tra team tecnici, management e stakeholder esterni, fungendo da strumento di governance oltre che di implementazione [^13]. La versione 2.0 introduce la funzione **Govern** come novità rispetto alle cinque funzioni originali, posizionando esplicitamente la cybersecurity come una scelta di governance aziendale — una decisione strategica del management — e non come un insieme di misure tecniche delegate ai soli specialisti. Le funzioni **Identify** e **Protect** corrispondono direttamente alle attività di patch management: la prima comprende l'inventario degli asset e la discovery delle vulnerabilità, la seconda include il deployment delle patch e l'implementazione dei controlli compensativi. Le funzioni **Detect**, **Respond** e **Recover** coprono invece il monitoraggio delle vulnerabilità non ancora patchate, la gestione degli incidenti che ne derivano e il ripristino operativo successivo. Il NIST SP 800-40r4 integra esplicitamente il patch management nel framework CSF, classificando i processi di patching come controlli primari nelle funzioni Identify e Protect, e i processi di monitoring delle vulnerabilità non risolte come componenti della funzione Detect [^2].

> **Figura consigliata — Fig. 3.3**: Diagramma circolare del ciclo RMF con le 7 fasi e le frecce che ne indicano la sequenzialità e il carattere iterativo. Una versione ufficiale di questa figura è disponibile gratuitamente sul sito NIST CSRC all'indirizzo **https://csrc.nist.gov/Projects/risk-management** nella sezione "RMF Overview". È preferibile utilizzare o adattare questa figura ufficiale, citando esplicitamente la fonte NIST SP 800-37r2 [^11], oppure ridisegnarla come diagramma a frecce con un layout circolare che enfatizzi il carattere ciclico e iterativo del processo.

Il principio unificante di tutti i framework NIST è il **risk-based approach**: le decisioni di sicurezza devono essere prese in funzione del rischio effettivo, non di obblighi di conformità formale. Questo paradigma implica che non tutte le vulnerabilità richiedano lo stesso livello di urgenza, che le risorse limitate vadano allocate in proporzione al rischio quantificato, e che le deroghe al patching immediato siano ammissibili purché documentate e accompagnate da controlli compensativi adeguati. Il NIST SP 800-30r1, pubblicazione fondamentale per la conduzione di valutazioni del rischio, fornisce la metodologia per quantificare il rischio come prodotto di *likelihood* — la probabilità che una minaccia si concretizzi — e *impact* — le conseguenze dell'evento sull'organizzazione — creando il fondamento matematico su cui si costruiscono le metriche composite che verranno descritte nelle sezioni successive [^14].

---

## 3.4 Le Strategie di Risk Treatment nel Contesto del Patch Management

Una volta che il rischio è stato identificato, valutato e quantificato secondo la metodologia descritta, l'organizzazione deve scegliere deliberatamente come rispondervi. Il NIST SP 800-30r1 e il NIST SP 800-39 definiscono quattro strategie fondamentali di *risk treatment* per i rischi negativi, ciascuna con implicazioni operative e organizzative distinte che nel contesto del patch management assumono declinazioni specifiche [^12][^14].

La prima strategia, denominata **Avoid**, consiste nel cessare l'attività che genera il rischio o nel dismettere il sistema che ne è fonte. Nel patch management, questa strategia si concretizza tipicamente nel *decommissioning* di un sistema legacy che non può più essere aggiornato in modo sicuro, o nella dismissione di un servizio applicativo per cui non esiste alcuna alternativa di remediation tecnicamente o economicamente praticabile. È la risposta più radicale al rischio — eliminandone la sorgente si elimina il rischio alla radice — ma comporta costi organizzativi spesso elevati: perdita di funzionalità operative, investimenti nella sostituzione, impatto sulla continuità dei processi di business. Per questa ragione, l'avoidance è la scelta appropriata principalmente quando il rischio supera significativamente il valore che l'organizzazione ricava dal sistema vulnerabile, o quando nessun'altra strategia consente di ridurre il rischio a livelli accettabili [^12].

La strategia di **Mitigate**, nettamente la più comune e articolata nella pratica operativa, mira a ridurre la probabilità di sfruttamento, l'impatto in caso di compromissione, o entrambi. L'applicazione della patch stessa rappresenta la forma primaria di mitigazione: chiude la vulnerabilità tecnica, eliminando il vettore di attacco senza incidere sulla funzionalità del sistema. Quando la patch non è disponibile o non può essere applicata, si ricorre a *controlli compensativi* che non risolvono la vulnerabilità ma ne riducono la sfruttabilità: la network segmentation e la micro-segmentation isolano il sistema vulnerabile limitando la superficie di attacco raggiungibile; Web Application Firewall (WAF) e sistemi IDS/IPS intercettano i tentativi di sfruttamento prima che raggiungano il sistema target; l'hardening della configurazione riduce la superficie esposta disabilitando servizi non necessari, limitando i privilegi e applicando le CIS Benchmarks [^15]; il *virtual patching*, tecnica particolarmente rilevante per i sistemi non aggiornabili, consiste nell'applicare regole specifiche agli strumenti di protezione perimetrale che neutralizzano il vettore di attacco della vulnerabilità senza modificare il sistema. È fondamentale comprendere che la mitigazione non elimina il rischio ma lo riduce a un livello residuo che deve comunque essere monitorato e documentato.

Il **Transfer** del rischio sposta la responsabilità finanziaria o operativa su un soggetto terzo. Le modalità principali nel contesto della cybersecurity includono la stipula di polizze di *cyber insurance*, che trasferisce l'esposizione finanziaria a una compagnia assicurativa, i contratti SLA con vendor che si assumono responsabilità contrattuali sulla tempestività delle patch e sul supporto in caso di incidente, e l'outsourcing della gestione a Managed Security Service Provider (MSSP). È essenziale rilevare che il trasferimento non riduce il rischio tecnico: il sistema rimane vulnerabile. Riduce l'esposizione finanziaria e di reputazione dell'organizzazione *in caso* di incidente, ma non la probabilità che questo accada. Inoltre, il trasferimento contrattuale non è accettabile per la responsabilità verso soggetti terzi i cui dati sono esposti: un'organizzazione non può trasferire contrattualmente la propria responsabilità verso gli interessati ai sensi del GDPR o di normative settoriali equivalenti in caso di data breach derivante da una vulnerabilità nota non gestita [^14].

L'**Accept** è la decisione consapevole di non intraprendere ulteriori azioni, tollerando il rischio residuo esistente. È la strategia appropriata quando il costo della mitigazione supera il valore del rischio ridotto, quando il rischio rientra nella *risk appetite* definita dall'organizzazione, o quando nessuna delle altre strategie è tecnicamente o economicamente praticabile. Il NIST RMF — attraverso la fase di Authorize — formalizza questo processo trasformando l'accettazione da omissione passiva a decisione formalmente documentata nel risk register, con l'indicazione esplicita del livello di rischio accettato, dell'autorità responsabile dell'approvazione, della data di calcolo e della data di revisione pianificata [^11][^12]. Il NIST IR 8286B chiarisce ulteriormente il ruolo del rischio accettato nella gestione del rischio d'impresa, sottolineando come la documentazione formale delle accettazioni di rischio sia un requisito di governance, non una formalità [^16].

---

## 3.5 Il Problema Strutturale dei Sistemi Non Patchabili

In ambienti IT reali, la decisione tra le quattro strategie descritte nella sezione precedente è raramente così netta come la classificazione teorica potrebbe suggerire. Esiste in particolare una categoria di situazioni in cui la strategia di mitigazione primaria — l'applicazione della patch — non è disponibile o non è praticabile, e le alternative comportano vincoli tecnici, economici e regolatori che rendono la scelta particolarmente complessa. Si tratta del problema dei **sistemi non patchabili**, una sfida strutturale e pervasiva che il NIST SP 800-40r4 riconosce esplicitamente come uno dei principali ostacoli all'efficacia dei programmi di patch management in contesti enterprise [^2].

Le categorie di sistemi non patchabili sono eterogenee per natura e per cause. I sistemi *legacy* fuori supporto — sistemi operativi e applicazioni per cui il vendor ha definitivamente cessato il rilascio di aggiornamenti di sicurezza — sono forse la categoria più numerosa: continuano a svolgere funzioni operative critiche per l'organizzazione, spesso perché la loro sostituzione richiederebbe investimenti proibitivi o la riscrittura di applicazioni che non esistono in versioni compatibili con sistemi moderni. Gli ambienti di controllo industriale (OT/ICS/SCADA) presentano un profilo di rischio particolarmente critico: l'aggiornamento di questi sistemi richiede finestre di manutenzione pianificate di giorni o settimane, con impatto diretto sulla produzione e sulla continuità del processo industriale, e in molti casi le certificazioni di sicurezza funzionale (come IEC 62443 o le normative specifiche dei settori energetico e farmaceutico) impongono che qualsiasi modifica al software sia sottoposta a nuova validazione, rendendo il ciclo di patching proibitivamente lungo e costoso. I dispositivi IoT ed embedded aggiungono una ulteriore dimensione del problema: il firmware può essere tecnicamente non aggiornabile senza la sostituzione fisica dell'hardware, oppure il processo di aggiornamento richiede un intervento fisico su larga scala geografica che supera il budget e le capacità operative del team di gestione. In altri casi ancora, il vincolo non è tecnico ma applicativo: una patch modifica il comportamento del sistema in modo incompatibile con dipendenze critiche verso altre applicazioni aziendali, richiedendo un aggiornamento a cascata i cui tempi e costi superano il valore della riduzione di rischio ottenuta.

In tutti questi scenari, il rischio persiste indefinitamente e deve essere gestito in modo proattivo e quantitativo. L'organizzazione si trova nella condizione di dover scegliere tra le strategie di Accept, Mitigate con controlli compensativi, Transfer o — nei casi estremi — Avoid, senza poter ricorrere alla soluzione tecnica primaria. La scelta non può essere arbitraria: deve essere fondata su una valutazione quantitativa del rischio residuo che consenta di confrontare sistemi non patchabili tra loro, di determinare dove concentrare le risorse limitate di protezione compensativa, e di documentare formalmente le accettazioni di rischio con una metrica oggettiva e difendibile nei confronti del management e degli auditor. È precisamente per rispondere a questa necessità che viene proposta la metrica descritta nella sezione successiva.

---

## 3.6 L'Insecurity Score: Architettura e Fondamenti Teorici di una Metrica Composita per il Rischio Residuo

L'**Insecurity Score** è una metrica composita progettata per quantificare il rischio residuo di un asset IT — tipicamente una macchina virtuale — che presenta vulnerabilità note e non patchate. La formula integra tre dimensioni complementari del rischio: la severità tecnica e la probabilità di sfruttamento delle vulnerabilità associate all'asset, e il contesto aziendale che ne determina l'esposizione e il valore per l'organizzazione. Il risultato è un punteggio normalizzato su scala da 0 a 100 che traduce dati tecnici — CVSS e EPSS, la cui interpretazione richiede competenza specialistica — in un numero immediatamente interpretabile da decision-maker non tecnici, rendendo possibile l'integrazione della metrica nel processo di risk management descritto nel framework NIST.

Il modello di rischio fondamentale su cui si basa la formula è quello classico della teoria del rischio informatico, formalizzato dal NIST SP 800-30r1, che esprime il rischio come prodotto di probabilità e impatto [^14]. In questo contesto specifico, la formulazione si estende per incorporare la dimensione del contesto aziendale:

$$\text{Insecurity Score} \approx \text{Severità}(\text{CVSS}) \times \text{Exploitability}(\text{EPSS}) \times \text{Contesto Aziendale}$$

La scelta di un **approccio moltiplicativo** anziché additivo non è arbitraria ma riflette una proprietà fondamentale del rischio informatico: i fattori di rischio si amplificano reciprocamente piuttosto che sommarsi linearmente. Una vulnerabilità critica su un sistema esposto a Internet con dati regolamentati non rappresenta la somma di tre rischi separati, ma una condizione in cui ciascun fattore amplifica l'effetto degli altri — analogamente a come, nella teoria delle probabilità condizionate, eventi correlati producono rischi sistemici superiori alla somma delle probabilità marginali. La struttura formale della formula è:

$$\text{Insecurity\_Score} = \min\!\left(100,\; \text{Technical\_Risk} \times \text{Context\_Multiplier} \times \text{Volume\_Multiplier}\right)$$

La funzione $\min(100, \cdot)$ garantisce che il punteggio rimanga nel range $[0, 100]$ anche in scenari estremi — dove tutti i fattori di rischio assumono il valore massimo simultaneamente — mantenendo la comparabilità tra tutti gli asset e la leggibilità del risultato. Un punteggio pari a 100 non indica "il caso peggiore in assoluto immaginabile" ma il superamento della soglia di massima urgenza: qualsiasi valore superiore satura all'identico livello, segnalando una condizione che richiede risposta immediata indipendentemente da ulteriori distinzioni.

### 3.6.1 La Componente di Rischio Tecnico

Il **Technical Risk** quantifica la pericolosità tecnica delle vulnerabilità associate all'asset attraverso una sommatoria ponderata che combina CVSS ed EPSS per ogni CVE presente:

$$\text{Technical\_Risk} = \frac{\sum_{i} \left( w_i \times \text{CVSS}_i \times (1 + \text{EPSS}_i) \right)}{20} \times 100$$

La combinazione $\text{CVSS} \times (1 + \text{EPSS})$ merita un'analisi specifica perché risolve un problema matematico non banale. La moltiplicazione diretta per EPSS — che varia tra 0 e 1 — azzererebbe il contributo del CVSS per vulnerabilità con probabilità di sfruttamento nulla, eliminando completamente la dimensione della severità tecnica in tutti i casi in cui EPSS = 0. Poiché EPSS misura la probabilità di sfruttamento *nei prossimi 30 giorni*, un valore nullo non indica che la vulnerabilità sia irrilevante, ma semplicemente che non vi sono indicazioni correnti di sfruttamento imminente. Il fattore $(1 + \text{EPSS})$ preserva il contributo della severità tecnica come baseline — con un moltiplicatore di 1.0 quando EPSS = 0 — e lo amplifica proporzionalmente alla probabilità di sfruttamento fino a un massimo di 2.0 quando EPSS = 1. Il denominatore 20 rappresenta il massimo teorico del prodotto $\text{CVSS} \times (1 + \text{EPSS})$, ottenuto quando CVSS = 10 ed EPSS = 1, garantendo che il Technical Risk sia sempre nel range $[0, 100]$ [^14].

Quando un asset presenta vulnerabilità multiple, la formula adotta una **ponderazione esponenziale decrescente** che assegna pesi diversi alle CVE in funzione della loro severità, ordinate per CVSS decrescente:

$$w_i = \frac{0.5^i}{\sum_j 0.5^j}$$

Con tre CVE, la distribuzione dei pesi è: 57.1% per la più grave, 28.6% per la seconda, 14.3% per la terza. Questo schema di ponderazione formalizza matematicamente il **principio del weakest link**: un attaccante razionale sfrutterà la vulnerabilità più grave disponibile, che offre il percorso di minor resistenza verso l'obiettivo. Le vulnerabilità secondarie offrono vie alternative, ma il rischio marginale che ciascuna aggiunge decresce in modo sistematico perché l'attaccante ha già a disposizione un vettore più efficiente. Il rischio complessivo è quindi dominato dalla debolezza principale, non dalla media di tutte le debolezze.

### 3.6.2 Il Contesto Aziendale come Fattore di Amplificazione

Il **Context Multiplier** trasforma il contesto aziendale dell'asset in un fattore di scala che amplifica o attenua il rischio tecnico calcolato nella componente precedente, seguendo la normalizzazione:

$$\text{Context\_Multiplier} = 0.5 + \frac{\text{Context\_Raw} - 0.125}{5.875}$$

dove $\text{Context\_Raw} = \text{Exposure\_Factor} \times \text{Criticality\_Factor} \times \text{DataClass\_Factor}$.

I tre fattori catturano dimensioni distinte e complementari del contesto: l'**Exposure Factor** (con valori 0.5 per sistemi interni, 1.0 per sistemi in DMZ e 1.5 per sistemi esposti a Internet) quantifica la raggiungibilità dell'asset da parte di un attaccante esterno; il **Criticality Factor** (0.5 per ambienti di sviluppo/test, 1.0 per workstation standard, 1.5 per application server, 2.0 per database e infrastruttura core) riflette l'importanza dell'asset per la continuità operativa dell'organizzazione; il **Data Classification Factor** (0.5 per dati pubblici, 1.0 per dati interni, 1.5 per dati riservati, 2.0 per dati critici o regolamentati) esprime la sensibilità delle informazioni gestite dall'asset. Il prodotto $\text{Context\_Raw}$ varia tra il minimo teorico di $0.5 \times 0.5 \times 0.5 = 0.125$ — corrispondente a un sistema interno, a bassa criticità e con dati pubblici — e il massimo teorico di $1.5 \times 2.0 \times 2.0 = 6.0$, corrispondente a un sistema esposto a Internet, critico e con dati regolamentati. La formula di normalizzazione lineare mappa questo intervallo $[0.125, 6.0]$ sul range $[0.5, 1.5]$ del moltiplicatore: nel caso più favorevole il rischio tecnico viene dimezzato, in quello più sfavorevole aumentato del 50%. Questa scelta di design riflette la realtà operativa ben documentata che la stessa identica vulnerabilità tecnica ha implicazioni completamente diverse su un server di sviluppo isolato rispetto a un database di pagamenti raggiungibile da Internet [^2][^14].

> **Figura consigliata — Fig. 3.4**: Heatmap con Exposure Factor sull'asse X (3 valori: Internal, DMZ, External) e Criticality Factor sull'asse Y (4 valori: Low, Medium, High, Critical), che mostra il valore del Context Multiplier per ogni combinazione con Data Classification fissata a "Confidential". Questa figura deve essere **creata originalmente** con Python/seaborn o R/ggplot2, in quanto è un contributo visivo originale della tesi che illustra la struttura del fattore contestuale. Il codice Python per generarla richiede una griglia 3×4 di valori calcolati dalla formula del Context Multiplier.

### 3.6.3 La Dimensione Volumetrica del Rischio

Il **Volume Multiplier** cattura l'effetto dell'accumulo di vulnerabilità multiple sullo stesso asset attraverso una crescita logaritmica:

$$\text{Volume\_Multiplier} = 1 + 0.1 \times \log_2(N_{\text{CVE}})$$

La funzione logaritmica modella il principio dei rendimenti decrescenti nell'accumulo di vulnerabilità: la seconda CVE introduce una nuova via d'attacco alternativa e rappresenta un incremento di rischio significativo, mentre la decima CVE si aggiunge in un contesto in cui l'attaccante dispone già di numerose opzioni e il rischio marginale è quindi limitato. Un asset con un'unica CVE critica risulta correttamente classificato come più rischioso di un asset con dieci CVE di severità moderata e bassa probabilità di sfruttamento, poiché il rischio complessivo è dominato dalla componente di Technical Risk — che a sua volta è dominata dalla vulnerabilità più grave attraverso la ponderazione esponenziale decrescente — e non dal semplice conteggio delle vulnerabilità presenti. Questa proprietà evita la deriva verso la quale i modelli additivi lineari sono intrinsecamente vulnerabili: il proliferare di CVE minori che, sommate, porterebbero un asset con molte vulnerabilità irrilevanti a superare un asset con una singola CVE critica.

### 3.6.4 Proprietà Matematiche e Scale di Interpretazione

Dal punto di vista matematico, l'Insecurity Score presenta tre proprietà fondamentali per la sua validità come strumento decisionale. La **monotonicità crescente** garantisce che ogni incremento in qualsiasi fattore di rischio — CVSS, EPSS, numero di CVE, esposizione, criticità, classificazione dati — produca un incremento del punteggio, senza comportamenti controintuitivi che potrebbero minare la fiducia nell'indicatore. La **boundedness** nel range $[0, 100]$ garantisce la comparabilità tra tutti gli asset e nel tempo, rendendo il punteggio utilizzabile come metrica di trend: un asset il cui score cresce nel tempo segnala un deterioramento della postura di sicurezza. L'**indipendenza** tra i punteggi dei diversi asset è una proprietà operativamente rilevante: aggiungere o rimuovere un asset dal dataset non modifica i punteggi degli altri asset, garantendo che i confronti temporali rimangano validi indipendentemente dall'evoluzione dell'inventario.

> **Figura consigliata — Fig. 3.5**: Diagramma a blocchi della formula che mostra le tre componenti (Technical Risk, Context Multiplier, Volume Multiplier) come rami paralleli che confluiscono nel prodotto moltiplicativo con saturazione a 100. Questa figura deve essere **creata originalmente** con strumenti come draw.io (disponibile su **https://app.diagrams.net**), Lucidchart, o direttamente in LaTeX con il pacchetto TikZ, in quanto rappresenta un contributo visivo originale della tesi. Non esiste una versione di questa figura in fonti esterne.

L'interpretazione operativa del punteggio è strutturata su cinque livelli di rischio che corrispondono a soglie di azione differenziate. Un punteggio nell'intervallo 80–100 indica un livello **CRITICAL** che richiede azione immediata e escalation al management, corrispondendo a scenari di compromissione imminente ad alta probabilità. L'intervallo 60–79 (**HIGH**) segnala la necessità di pianificare la remediation o l'implementazione di controlli compensativi entro il ciclo operativo corrente. Un punteggio 40–59 (**MEDIUM**) indica un rischio moderato da includere nella pianificazione ordinaria del patching. L'intervallo 20–39 (**LOW**) corrisponde a un rischio accettabile nel breve termine che richiede monitoraggio attivo ma non intervento immediato. Infine, punteggi inferiori a 20 (**MINIMAL**) indicano un rischio trascurabile che può essere gestito nella revisione del ciclo successivo.

---

## 3.7 L'Insecurity Score nel Processo Decisionale di Risk Treatment

L'Insecurity Score, in quanto strumento decisionale e non semplice indicatore di misurazione, trova la sua funzione più rilevante nell'integrazione con le strategie di risk treatment del framework NIST descritte nella sezione 3.4, e in particolare nei contesti in cui la patch non può essere applicata — lo scenario centrale di questo lavoro. La formula traduce una costellazione di dati tecnici eterogenei in un singolo numero normalizzato che costituisce una lingua comune tra specialisti di sicurezza, team operativi e decision-maker aziendali, rendendo possibile l'integrazione della valutazione del rischio nei processi di governance.

Il mapping tra punteggio e strategia di risk treatment non è rigido ma orientativo, e deve essere calibrato sulla *risk appetite* specifica di ciascuna organizzazione così come definita nella fase di Prepare del NIST RMF [^11]. In linea generale, un asset non patchabile con punteggio nel range CRITICAL non è candidato all'accettazione del rischio all'interno della normale *risk appetite* aziendale: la formula fornisce l'evidenza quantitativa e documentabile per escalare la decisione al management e valutare strategie di Avoid — dismettendo o isolando il sistema — o di Transfer del rischio attraverso strumenti assicurativi o contrattuali. Per punteggi nel range HIGH e MEDIUM, la strategia appropriata è tipicamente la Mitigate attraverso controlli compensativi, dove la formula fornisce il criterio di prioritizzazione: in presenza di risorse limitate per l'implementazione di network segmentation, virtual patching o sistemi IDS, gli asset con punteggio più elevato ricevono la precedenza assoluta, indipendentemente da altre considerazioni. Per punteggi LOW e MINIMAL, l'Accept con monitoraggio attivo è la risposta proporzionale al rischio identificato, e il punteggio diventa la metrica formale da documentare nel risk register.

È in questa ultima funzione che l'Insecurity Score esprime forse il proprio contributo più significativo per l'infrastruttura di governance della sicurezza. Ogni asset non patchabile con punteggio documentato diventa una voce formale nel risk register dell'organizzazione, con l'indicazione dei parametri di input — CVSS e EPSS delle CVE presenti, fattori di contesto — della data di calcolo e della data di revisione pianificata. Questo processo corrisponde esattamente alla fase di Authorize del NIST RMF [^11]: una decisione consapevole, fondata su evidenza quantitativa, formalmente approvata dall'autorità responsabile. La differenza rispetto all'accettazione implicita — lasciare semplicemente un sistema non patchato senza documentazione — è cruciale sia dal punto di vista della governance che della responsabilità legale: un'organizzazione che può dimostrare di aver identificato, quantificato e formalmente accettato un rischio residuo si trova in una posizione radicalmente diversa rispetto a una che ha semplicemente omesso di gestirlo [^16].

---

## Riferimenti Bibliografici

[^1]: Ojuri, M. A. (2021). *Evaluating Cybersecurity Patch Management through QA Performance Indicators*. International Journal of Technology Management & Humanities (IJTMH), 7(4), December 2021.

[^2]: Souppaya, M., & Scarfone, K. (2022). *Guide to Enterprise Patch Management Planning: Preventive Maintenance for Technology* (NIST SP 800-40 Revision 4). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-40r4.pdf

[^3]: SentinelOne. (2024). *Vulnerability Management Metrics: 20 Key KPIs to Track*. https://www.sentinelone.com/cybersecurity-101/cybersecurity/vulnerability-management-metrics/

[^4]: NinjaOne. (2024). *10 Essential Patch Management Metrics*. https://www.ninjaone.com/blog/patch-management-metrics/

[^5]: FIRST. (2023). *Common Vulnerability Scoring System v4.0: Specification Document*. Forum of Incident Response and Security Teams. https://www.first.org/cvss/v4-0/

[^6]: Cloudsmith. (2024). *CVSS vs EPSS: Smarter Vulnerability Risk Prioritization*. https://cloudsmith.com/blog/vulnerability-scoring-systems

[^7]: Attaxion. (2024). *CVSS vs. EPSS: What's the Difference?* https://attaxion.com/blog/cvss-vs-epss-difference/

[^8]: FIRST. (2025). *Exploit Prediction Scoring System (EPSS) Model v4*. Forum of Incident Response and Security Teams. https://www.first.org/epss/

[^9]: Picus Security. (2024). *Comparing CVSS, EPSS, KEV, SSVC, LEV, and PXS: From Scores to Security Proof*. https://www.picussecurity.com/resource/blog/comparing-cvss-epss-kev-ssvc-lev-and-pxs-from-scores-to-security-proof

[^10]: CISA. (2024). *Known Exploited Vulnerabilities Catalog*. Cybersecurity and Infrastructure Security Agency. https://www.cisa.gov/known-exploited-vulnerabilities-catalog

[^11]: Joint Task Force. (2018). *Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy* (NIST SP 800-37 Revision 2). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-37r2.pdf

[^12]: Joint Task Force Transformation Initiative. (2011). *Managing Information Security Risk: Organization, Mission, and Information System View* (NIST SP 800-39). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-39.pdf

[^13]: NIST. (2024). *The NIST Cybersecurity Framework 2.0* (NIST CSWP 29). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.29.pdf

[^14]: Joint Task Force Transformation Initiative. (2012). *Guide for Conducting Risk Assessments* (NIST SP 800-30 Revision 1). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-30r1.pdf

[^15]: CIS. (2024). *CIS Controls v8.1*. Center for Internet Security. https://www.cisecurity.org/controls/

[^16]: Ross, R., Pillitteri, V., Graubart, R., Bodeau, D., & McQuaid, R. (2022). *Prioritizing Cybersecurity Risk for Enterprise Risk Management* (NIST IR 8286B). National Institute of Standards and Technology. https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8286B.pdf
