- [ ] Active Directory
- [ ] OpenSCAP
- [ ] Metter i servizi in Alta Affidabilit√† 
	- [ ] Proporne un architettura
- [ ] **NO** - POC  Su ambienti MASE
- [ ] **NO** - Verifica Update proxy mgrpxy

## Active Directory

Il tuo account Azure non ha il ruolo Application Administrator o superiore 
- Si pu√≤ configura SAML in Uyuni 
- Role Mapping automatico via Gruppi AD
- Accesso root al server Uyuni (via Azure Bastion)

## OpenSCAP

- Seleziona il profilo di benchmark **CIS (Center for Internet Security)** per il sistema operativo target
- Applica le regole di hardening e i controlli di sicurezza CIS
- Genera un report di conformit√† (pass/fail per ogni regola)
- Pu√≤ anche produrre remediation script (fix automatici), se richiesto
## Riferito al tuo file

Path:
`/usr/share/xml/scap/ssg/content/ssg-rhel9-ds.xml`

Questo √® il DataStream SCAP del progetto **SCAP Security Guide** per **Red Hat Enterprise Linux 9**.
Quindi il comando sta dicendo:

üëâ ‚ÄúValuta il sistema RHEL 9 secondo il benchmark CIS ufficiale.‚Äù
## In sintesi
Produce:
‚û°Ô∏è una scansione di conformit√† CIS del sistema  
‚û°Ô∏è un report di sicurezza basato su benchmark ufficiali  
‚û°Ô∏è eventuali suggerimenti di hardening

## Alta Affidabilit√†

> **UYUNI non supporta clustering active/active nativo del server.**
> La documentazione ufficiale SUSE/UYUNI definisce un solo pattern HA supportato per il server: **Active/Passive** con failover manuale o semiautomatico. Lo stato dei client risiede interamente nel database PostgreSQL del server primario.


Il proxy UYUNI (`mgr-proxy`) √® per definizione stateless: non ha database proprio, non ha Salt Master proprio. Lo stato di ogni client gestito via proxy vive sul server centrale. Questo √® un vincolo architetturale che per√≤ diventa un vantaggio per l'HA del proxy.


UYUNI Server ‚Äî Active/Passive HA
### Architettura

Il server UYUNI √® il componente con il vincolo architetturale pi√π rigido: non pu√≤ essere attivo su pi√π nodi contemporaneamente. Il pattern adottato √® quindi **active/passive con warm standby**:

- **Primary (AZ1)**: server UYUNI attivo, database PostgreSQL primario
- **Standby (AZ2)**: server UYUNI fermo (`mgradm stop`), database PostgreSQL in hot standby con streaming replication live

Lo standby √® "warm" nel senso che il database √® sincronizzato in tempo reale e pronto alla promozione, ma il processo applicativo UYUNI (Tomcat, Taskomatic, Salt Master) rimane fermo fino al failover. Questo √® necessario perch√© UYUNI non √® progettato per avere due istanze attive sullo stesso database.

**RPO**: < 1 secondo con `synchronous_commit = on` | < 1 minuto con `synchronous_commit = local`
**RTO**: 10-12 minuti con operatore disponibile (procedura documentata nella sezione 10.1)

### Perch√© il processo UYUNI √® fermato sullo standby

Il server UYUNI (Tomcat + Taskomatic + Salt Master) non √® progettato per la coesistenza multi-nodo. Due istanze UYUNI attive sullo stesso database causerebbero:
- Conflitti di scheduling su Taskomatic (doppia esecuzione di job)
- Conflitti sul Salt Master (due master per gli stessi minion)
- Corruzione della coda di eventi Salt

L'unico approccio sicuro √® mantenere una sola istanza applicativa attiva. Il database invece pu√≤ (e deve) essere in replication in tempo reale per garantire RPO basso.

### PostgreSQL Streaming Replication

La replication √® configurata a livello di container `uyuni-db`. Il parametro `synchronous_commit` √® una scelta di bilanciamento tra RPO e performance:

- `synchronous_commit = on`: il commit ritorna al client solo quando la scrittura √® confermata sul WAL standby ‚Üí RPO=0, impatto ~15% sulle performance di scrittura
- `synchronous_commit = local`: il commit ritorna al client dopo scrittura locale ‚Üí RPO < 1s, nessun impatto performance

Per un sistema di patch management la scelta raccomandata √® `synchronous_commit = local`: un secondo di transazioni perse in caso di failure √® accettabile, e l'impatto performance su `synchronous_commit = on` degrada l'esperienza operativa quotidiana.


### Storage `/manager_storage` ‚Äî Azure Files NFS Premium ZRS

Il repository dei pacchetti (`/manager_storage`) contiene centinaia di GB di dati binari (RPM, DEB, metadati canali). Non √® replicabile via PostgreSQL. La soluzione √® uno storage condiviso montato su entrambi i nodi.

**Scelta**: Azure Files NFS Premium con ridondanza ZRS (Zone-Redundant Storage). Il mount √® presente su entrambi i nodi (primary e standby). In failover, il nodo standby trova lo storage gi√† montato e aggiornato ‚Äî non √® necessario alcun trasferimento dati.


### Active/Passive per pair ‚Äî motivazione

Il Salt minion mantiene due connessioni TCP persistenti verso il proxy:
- Porta 4505 (publish port): il master invia comandi, il minion ascolta
- Porta 4506 (return port): il minion risponde, il master ascolta

Queste connessioni sono **long-lived** e stateful a livello Salt (il minion si autentica al Salt Master attraverso il proxy). Se HAProxy distribuisse le connessioni in round-robin tra due proxy VM (active/active), connessioni successive dello stesso minion potrebbero arrivare su VM diverse, ciascuna con il proprio stato del broker Salt ‚Äî risultato: sessione corrotta o minion non raggiungibile.

Il modello active/passive √® la soluzione corretta:
- Un solo proxy VM riceve tutto il traffico dell'organizzazione in condizioni normali
- Failover automatico all'altro nodo solo in caso di failure
- Il minion subisce un TCP reset e si riconnette al nodo ora attivo ‚Äî comportamento nativo Salt, nessuna configurazione client richiesta


# A cosa servono davvero Load Balancer e HAProxy

## üéØ Ruoli distinti (non sono la stessa cosa)

### üîµ Azure Load Balancer (ILB)

Serve per:

- fornire un **VIP** (Virtual IP)
    
- health probe
    
- failover L4
    
- distribuire traffico verso pi√π VM
    

üëâ √à infrastrutturale (Layer 4)

Non termina SSL  
Non fa routing applicativo  
Non conosce Uyuni

---

### üü¢ HAProxy

Serve per:

- reverse proxy applicativo
    
- SSL termination
    
- routing verso Primary/Standby
    
- health check HTTP `/rpc`
    

üëâ √à applicativo (Layer 7)

Conosce lo stato dei nodi Uyuni.

Azure LB da solo NON pu√≤:

- fare failover applicativo
    
- sapere chi √® Primary Uyuni
    
- terminare SSL con logica avanzata
    

HAProxy s√¨.