- [ ] Active Directory
- [ ] OpenSCAP
- [ ] Metter i servizi in Alta Affidabilità 
	- [ ] Proporne un architettura
- [ ] **NO** - POC  Su ambienti MASE
- [ ] **NO** - Verifica Update proxy mgrpxy

## SSL Certificate

Il canale Salt (4505/4506) è completamente separato — usa le sue chiavi RSA, non TLS con certificati CA. Quel sistema  non lo tocchi.

Dove va il certificato nel deployment iniziale

1. **Server Uyuni** — mgradm install podman

 Al momento dell'installazione puoi fare due cose:

#### Opzione A — Uyuni genera tutto da solo (default):
mgradm install podman
Crea una CA self-signed interna in /root/ssl-build/ dentro il container. Nessun certificato tuo entra.

#### Opzione B — Porti il tuo certificato firmato dalla tua CA:
  mgradm install podman \
    --ssl-ca-cert    /path/to/ca.pem          # Root CA (o chain)
    --ssl-server-cert /path/to/server.crt      # Cert del server firmato dalla CA
    --ssl-server-key  /path/to/server.key      # Chiave privata del server

  In entrambi i casi il server espone il certificato su porta 443. La differenza è solo chi ha firmato quel certificato.

2. **Proxy Uyuni** — mgradm proxy
  Il proxy ha bisogno di tre cose:

  3. CA certificate     → per verificare il Server Uyuni a cui si connette
  4. Proxy certificate  → il suo certificato HTTPS (firmato dalla stessa CA)
  5. Proxy private key  → la chiave privata corrispondente

 Nella pratica con mgradm proxy generate-config o via Web UI (Admin → Proxy → Generate Configuration) Uyuni ti genera un pacchetto di configurazione che include già il certificato proxy firmato dalla CA Uyuni interna. Se invece hai una CA esterna, devi passare i file manualmente.

Punto chiave: il proxy si registra su Uyuni Server e l'identità che conta è il FQDN. Come hai documentato in HA-Architecture-UYUNI.md, se usi un VIP, il FQDN del proxy deve corrispondere al VIP, e entrambi i nodi del pair
  devono avere gli stessi certificati.

3. **Salt Minion** (client gestiti) — nessuna azione manuale

Questo è il punto che probabilmente ti stavi chiedendo: non devi fare nulla manualmente sui client gestiti.

  mgr-bootstrap.sh         ← script generato da Uyuni
      │
      ├── installa salt-minion
      ├── configura /etc/salt/minion con l'IP del master
      └── installa RHN-ORG-TRUSTED-SSL-CERT nel trust store del sistema
          (es. /etc/ssl/certs/ su Ubuntu, /etc/pki/ca-trust/ su RHEL)

Uyuni include automaticamente il file CA nel bootstrap script. Quando un client esegue mgr-bootstrap.sh o si registra  tramite Activation Key, riceve già la CA cert e la installa nel sistema. Il minion si fida automaticamente del server per i download HTTPS dei pacchetti.

Schema riassuntivo — chi riceve cosa

  LA TUA ROOT CA (privata)
          │
          ├──── FIRMI ────► Certificato Server Uyuni
          │                 └─ passi a mgradm install podman
          │
          ├──── FIRMI ────► Certificato Proxy Uyuni
          │                 └─ passi a mgradm proxy
          │
          └──── CA cert ──► da distribuire a:
                            ├── Browser operatori (manuale/GPO)
                            ├── SPM Orchestrator VM (una volta)
                            │   → /etc/ssl/certs/ + UYUNI_VERIFY_SSL=true
                            └── Salt Minion (AUTOMATICO via bootstrap)
                                → già incluso in mgr-bootstrap.sh

## Active Directory

Il tuo account Azure non ha il ruolo Application Administrator o superiore 
- Si può configura SAML in Uyuni 
- Role Mapping automatico via Gruppi AD
- Accesso root al server Uyuni (via Azure Bastion)

## OpenSCAP

- Seleziona il profilo di benchmark **CIS (Center for Internet Security)** per il sistema operativo target
- Applica le regole di hardening e i controlli di sicurezza CIS
- Genera un report di conformità (pass/fail per ogni regola)
- Può anche produrre remediation script (fix automatici), se richiesto
## Riferito al tuo file

Path:
`/usr/share/xml/scap/ssg/content/ssg-rhel9-ds.xml`

Questo è il DataStream SCAP del progetto **SCAP Security Guide** per **Red Hat Enterprise Linux 9**.
Quindi il comando sta dicendo:

- “Valuta il sistema RHEL 9 secondo il benchmark CIS ufficiale.”
## In sintesi
Produce:
- una scansione di conformità CIS del sistema  
- un report di sicurezza basato su benchmark ufficiali  
- eventuali suggerimenti di hardening

## Alta Affidabilità

> **UYUNI non supporta clustering active/active nativo del server.**
> La documentazione ufficiale SUSE/UYUNI definisce un solo pattern HA supportato per il server: **Active/Passive** con failover manuale o semiautomatico. Lo stato dei client risiede interamente nel database PostgreSQL del server primario.


Il proxy UYUNI (`mgr-proxy`) è per definizione stateless: non ha database proprio, non ha Salt Master proprio. Lo stato di ogni client gestito via proxy vive sul server centrale. Questo è un vincolo architetturale.

UYUNI Server — Active/Passive HA
### Architettura

Il server UYUNI è il componente con il vincolo architetturale più rigido: non può essere attivo su più nodi contemporaneamente. Il pattern adottato è quindi **active/passive con warm standby**:

- **Primary (AZ1)**: server UYUNI attivo, database PostgreSQL primario
- **Standby (AZ2)**: server UYUNI fermo (`mgradm stop`), database PostgreSQL in hot standby con streaming replication live

Lo standby è "warm" nel senso che il database è sincronizzato in tempo reale e pronto alla promozione, ma il processo applicativo UYUNI (Tomcat, Taskomatic, Salt Master) rimane fermo fino al failover. Questo è necessario perché UYUNI non è progettato per avere due istanze attive sullo stesso database.

**RPO**: < 1 secondo con `synchronous_commit = on` | < 1 minuto con `synchronous_commit = local`
**RTO**: 10-12 minuti con operatore disponibile (procedura documentata nella sezione 10.1)

### Perché il processo UYUNI è fermato sullo standby

Il server UYUNI (Tomcat + Taskomatic + Salt Master) non è progettato per la coesistenza multi-nodo. Due istanze UYUNI attive sullo stesso database causerebbero:
- Conflitti di scheduling su Taskomatic (doppia esecuzione di job)
- Conflitti sul Salt Master (due master per gli stessi minion)
- Corruzione della coda di eventi Salt

L'unico approccio sicuro è mantenere una sola istanza applicativa attiva. Il database invece può (e deve) essere in replication in tempo reale per garantire RPO basso.

### PostgreSQL Streaming Replication
La replication è configurata a livello di container `uyuni-db`. Il parametro `synchronous_commit` è una scelta di bilanciamento tra RPO e performance:

- `synchronous_commit = on`: il commit ritorna al client solo quando la scrittura è confermata sul WAL standby → RPO=0, impatto ~15% sulle performance di scrittura
- `synchronous_commit = local`: il commit ritorna al client dopo scrittura locale → RPO < 1s, nessun impatto performance

Per un sistema di patch management la scelta raccomandata è `synchronous_commit = local`: un secondo di transazioni perse in caso di failure è accettabile, e l'impatto performance su `synchronous_commit = on` degrada l'esperienza operativa quotidiana.

### Storage `/manager_storage` — Azure Files NFS Premium ZRS

Il repository dei pacchetti (`/manager_storage`) contiene centinaia di GB di dati binari (RPM, DEB, metadati canali). Non è replicabile via PostgreSQL. La soluzione è uno storage condiviso montato su entrambi i nodi.

**Scelta**: Azure Files NFS Premium con ridondanza ZRS (Zone-Redundant Storage). Il mount è presente su entrambi i nodi (primary e standby). In failover, il nodo standby trova lo storage già montato e aggiornato — non è necessario alcun trasferimento dati.


### Active/Passive per pair — motivazione

Il Salt minion mantiene due connessioni TCP persistenti verso il proxy:
- Porta 4505 (publish port): il master invia comandi, il minion ascolta
- Porta 4506 (return port): il minion risponde, il master ascolta

Queste connessioni sono **long-lived** e stateful a livello Salt (il minion si autentica al Salt Master attraverso il proxy). Se HAProxy distribuisse le connessioni in round-robin tra due proxy VM (active/active), connessioni successive dello stesso minion potrebbero arrivare su VM diverse, ciascuna con il proprio stato del broker Salt — risultato: sessione corrotta o minion non raggiungibile.

Il modello active/passive è la soluzione corretta:
- Un solo proxy VM riceve tutto il traffico dell'organizzazione in condizioni normali
- Failover automatico all'altro nodo solo in caso di failure
- Il minion subisce un TCP reset e si riconnette al nodo ora attivo — comportamento nativo Salt, nessuna configurazione client richiesta


# A cosa servono davvero Load Balancer e HAProxy

## Ruoli distinti (non sono la stessa cosa)

### Azure Load Balancer (ILB)
Serve per:
- fornire un **VIP** (Virtual IP)
- health probe
- failover L4
- distribuire traffico verso più VM

**È infrastrutturale (Layer 4)**

Non termina SSL  
Non fa routing applicativo  
Non conosce Uyuni
### HAProxy
Serve per:
- reverse proxy applicativo
- SSL termination
- routing verso Primary/Standby
- health check HTTP `/rpc`

**È applicativo (Layer 7)**

Conosce lo stato dei nodi Uyuni.
Azure LB da solo NON può:
- fare failover applicativo
- sapere chi è Primary Uyuni
- terminare SSL con logica avanzata

HAProxy sì.